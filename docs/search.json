[
  {
    "objectID": "hard_to_classify_datapoints.html",
    "href": "hard_to_classify_datapoints.html",
    "title": "Hard to classify datapoints",
    "section": "",
    "text": "|      |   y |   proba |\n|-----:|----:|--------:|\n|  460 |   1 |    0.83 |\n| 1287 |   1 |    0.84 |\n|  674 |   1 |    0.86 |\n| 1468 |   1 |    0.9  |\n|  916 |   1 |    0.93 |\n\n\n\n\n\n\n\n\n\n\n\ny\nproba\n\n\n\n\n460\n1\n0.83\n\n\n1287\n1\n0.84\n\n\n674\n1\n0.86\n\n\n1468\n1\n0.90\n\n\n916\n0\n0.93\n\n\n\n\n\n\n\n\n\n|      |   y |   proba |\n|-----:|----:|--------:|\n|  460 |   1 |    0.83 |\n| 1287 |   1 |    0.84 |\n|  674 |   1 |    0.86 |\n| 1468 |   1 |    0.9  |\n|  916 |   0 |    0.93 |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny\nproba\n\n\n\n\n460\n1\n0.83\n\n\n1287\n1\n0.84\n\n\n674\n1\n0.86\n\n\n1468\n0\n0.90\n\n\n916\n0\n0.93\n\n\n\n\n\n\n\n\n\n|      |   y |   proba |\n|-----:|----:|--------:|\n|  460 |   1 |    0.83 |\n| 1287 |   1 |    0.84 |\n|  674 |   1 |    0.86 |\n| 1468 |   0 |    0.9  |\n|  916 |   0 |    0.93 |"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#introduction",
    "href": "posts/hyperparameter_tuning_spark.html#introduction",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Introduction",
    "text": "Introduction\nHyperparameter tuning of machine learning models often requires significant computing time. Scikit-learn implements parallel processing to speed things up, but real speed gain can only be achieved by applying distributed computing like using Spark. In this blog post I show how to do hyperparameter tuning in Spark for any machine learning model, independent whether it’s scikit-learn, Tensorflow or Keras, XGBoost, LightGBM etc."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "href": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Create combinations of hyperparameter values",
    "text": "Create combinations of hyperparameter values\nFirst, we’re going to create hyperparameter combinations that we want to test our model for. Below is a helper functions to create all combinations for a param_grid that contains the arguments and the values to test for.\n\nimport numpy as np\nfrom itertools import product\n\n\ndef create_hyperparameter_combinations(param_grid):\n    combinations = list(product(*param_grid.values()))\n    return [dict(zip(param_grid.keys(), x)) for x in combinations]\n\nSo if our desired hyperparameter space is as follows…\n\nparam_grid = {'max_features': ['auto', 0.1, 0.3], 'min_samples_leaf': [None, 50]}\n\n…then the combination of these two hyperparameters and their values is obtained by`:\n\ncreate_hyperparameter_combinations(param_grid)\n\n[{'max_features': 'auto', 'min_samples_leaf': None},\n {'max_features': 'auto', 'min_samples_leaf': 50},\n {'max_features': 0.1, 'min_samples_leaf': None},\n {'max_features': 0.1, 'min_samples_leaf': 50},\n {'max_features': 0.3, 'min_samples_leaf': None},\n {'max_features': 0.3, 'min_samples_leaf': 50}]"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "href": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Create evaluation function",
    "text": "Create evaluation function\nNow, we’re going to create a function that takes a single combination of hyperparameter values and returns performance metrics for train and test data. This function will be evaluated in distributed fashion on our Spark cluster. If you want to test 500 different hyperparameter combinations, you will see 500 jobs being executed by Spark. In this example , were going to optimise hyperparameters for a scikit-learn model. This requires scikit-learn to be installed on the worker nodes of your Spark cluster.\nThe hyperparameter values for a particular combination are provided to the function as a json, so a string type. The advantage of this is that we don’t need to change the function definition if we want to add hyperparameters. Moreover, we are independent of the type of hyperparameter values. Many hyperparameters in scikit-learn take different types like integers, floats and strings like for example max_features in the RandomForestClassifier. When we provide them as a string containing a json, Spark never complains.\nThe function definition is as follows, explanation continues below.\n\nimport json\n\nfrom sklearn.model_selection import cross_validate\n\ndef evaluate_clf(base_clf, hyperpars_json, X, y, cv=5):\n    hyperpars = json.loads(hyperpars_json)\n    base_clf.set_params(**hyperpars)\n    cv_results = cross_validate(base_clf, X, y, cv=cv, return_train_score=True)\n    return (hyperpars_json,\n            float(np.mean(cv_results['train_score'])),\n            float(np.mean(cv_results['test_score'])))\n\nIn the first line, we convert the single combination of hyperparameter values to a Python dict by using json.loads. The next line sets these parameters in our scikit-learn model called base_clf. The scikit-learn function cross_validate takes the model, our training data features X and target y to produce train and test scores using cross validation. This function returns the results in a dict of which we take the train_score and test_score values that we return along with the hyperpars_json that we entered. Since Spark sometimes has difficulties with the np.float64 type, we convert the scores to the float type."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "href": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Distribute the evaluation function in Spark",
    "text": "Distribute the evaluation function in Spark\nThe next step is to distribute the hyperparameter combinations and use our evaluation function to calculate model performance metrics for these hyperparameter values. This is done in the function below.\n\nfrom pyspark.sql import types as T\n\ndef get_hyperparameter_results(spark_session, base_clf, X, y, \n                               hyperpar_combinations, cv=5):\n    hyperpar_combinations_json = [json.dumps(x) for x in hyperpar_combinations]\n    hyperpars_rdd = spark_session.sparkContext.parallelize(hyperpar_combinations_json, \n                                                           len(hyperpar_combinations_json))\n\n    rdd_map_result = hyperpars_rdd.map(lambda x: evaluate_clf(base_clf, x, X, y, cv))\n\n    result_schema = T.StructType([T.StructField('hyperpars', T.StringType()),\n                                  T.StructField('mean_train_score', T.FloatType()),\n                                  T.StructField('mean_test_score', T.FloatType()),\n                                  ])\n\n    result_sdf = spark_session.createDataFrame(rdd_map_result, schema=result_schema)\n    result = (result_sdf.toPandas()\n              .sort_values('mean_test_score', ascending=False)\n              .reset_index(drop=True))\n\n    result['hyperpars'] = result['hyperpars'].apply(json.loads)\n    return result\n\nIn the first line we convert the dicts to json strings. Then, we parallelize these jsons by creating an RDD. The evaluate_clf can is mapped to this RDD. The schema of the result is defined as a StructType containing StructFields. Then a Spark dataframe is created that hold the results of our hyperparameter tuning. We convert this Spark dataframe to a Pandas dataframe in order to explore the results easily. As a last step we convert the jsons back to dicts."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "href": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Full working example",
    "text": "Full working example\nIn the example below we do hyperparameter tuning of a DecisionTreeClassifier to predict classes for the famous iris dataset.\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_iris(return_X_y=True)\n\nparam_grid = {'max_depth':[3,4,5,10], 'min_samples_leaf':[0.1, 5, 10]}\n\nhyperpar_combinations = create_hyperparameter_combinations(param_grid)\n\nresults = get_hyperparameter_results(spark, DecisionTreeClassifier(), X, y, \n                                     hyperpar_combinations, cv=5)\nresults\n\n\n\n\n\n\n\n\nhyperpars\nmean_train_score\nmean_test_score\n\n\n\n\n0\n{'max_depth': 3, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n1\n{'max_depth': 4, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n2\n{'max_depth': 5, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n3\n{'max_depth': 10, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n4\n{'max_depth': 3, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n5\n{'max_depth': 3, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n6\n{'max_depth': 4, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n7\n{'max_depth': 4, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n8\n{'max_depth': 5, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n9\n{'max_depth': 5, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n10\n{'max_depth': 10, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n11\n{'max_depth': 10, 'min_samples_leaf': 10}\n0.961667\n0.933333"
  },
  {
    "objectID": "posts/Deduplipy.html",
    "href": "posts/Deduplipy.html",
    "title": "Deduplication of records using DedupliPy",
    "section": "",
    "text": "Deduplication or entity resolution is the task to combine different representations of the same real world entity. The Python package DedupliPy implements deduplication using active learning. Active learning allows for rapid training without having to provide a large, manually labelled dataset. In this post I demonstrate how the package works and show more advanced settings."
  },
  {
    "objectID": "posts/Deduplipy.html#installation",
    "href": "posts/Deduplipy.html#installation",
    "title": "Deduplication of records using DedupliPy",
    "section": "Installation",
    "text": "Installation\nDedupliPy can simply be installed from PyPi. Just type the following in the command line:\npip install deduplipy"
  },
  {
    "objectID": "posts/Deduplipy.html#simple-deduplication",
    "href": "posts/Deduplipy.html#simple-deduplication",
    "title": "Deduplication of records using DedupliPy",
    "section": "Simple deduplication",
    "text": "Simple deduplication\nDedupliPy comes with example data. We first load the ‘voters’ data that contains duplicate records:\n\nfrom deduplipy.datasets import load_data\n\ndf = load_data(kind='voters')\n\nColumn names: 'name', 'suburb', 'postcode'\n\n\nThis dataset contains names, suburbs and postcodes.\n\ndf.head(2)\n\n\n\n\n\n\n\n\nname\nsuburb\npostcode\n\n\n\n\n0\nkhimerc thomas\ncharlotte\n2826g\n\n\n1\nlucille richardst\nkannapolis\n28o81\n\n\n\n\n\n\n\nCreate a Deduplicator instance and provide the column names to be used for deduplication:\n\nfrom deduplipy.deduplicator import Deduplicator\n\n\nmyDedupliPy = Deduplicator(['name', 'suburb', 'postcode'])\n\nFit the Deduplicator by active learning; enter whether a pair is a match (y) or not (n). When the training is converged, you will be notified and you can finish training by entering ‘f’.\n\nmyDedupliPy.fit(df)\n\nApply the trained Deduplicator on (new) data. The column deduplication_id is the identifier for a cluster. Rows with the same deduplication_id are found to be the same real world entity.\n\nres = myDedupliPy.predict(df)\nres.sort_values('deduplication_id').head(10)\n\n\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\n1048\npriscilla amrtin\ncharlotte\n28212\n1\n\n\n381\npriscilla martin\ncharlotte\n28210\n1\n\n\n1305\ncharlotte mccraney\ncharl0tte\n28276\n2\n\n\n328\ncharlotte mccraney\ncharlotte\n28216\n2\n\n\n123\ncaria macartney\ncharlotte\n28220\n3\n\n\n1447\ncarla macartney\ncharlotte\n28227\n3\n\n\n1740\nwalter monaan\ncharlotte\n28202\n4\n\n\n268\nwalter monahan\ncharlotte\n28205\n4\n\n\n1380\nkiea matthews\ncharlotte\n28218\n5\n\n\n252\nkiera matthews\ncharlotte\n28216\n5\n\n\n\n\n\n\n\nThe Deduplicator instance can be saved as a pickle file and be applied on new data after training:\n\nimport pickle\n\n\nwith open('mypickle.pkl', 'wb') as f:\n    pickle.dump(myDedupliPy, f)\n\n\nwith open('mypickle.pkl', 'rb') as f:\n    loaded_obj = pickle.load(f)\n\n\nres = loaded_obj.predict(df)\nres.sort_values('deduplication_id').head(10)\n\n\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\n1048\npriscilla amrtin\ncharlotte\n28212\n1\n\n\n381\npriscilla martin\ncharlotte\n28210\n1\n\n\n1305\ncharlotte mccraney\ncharl0tte\n28276\n2\n\n\n328\ncharlotte mccraney\ncharlotte\n28216\n2\n\n\n123\ncaria macartney\ncharlotte\n28220\n3\n\n\n1447\ncarla macartney\ncharlotte\n28227\n3\n\n\n1740\nwalter monaan\ncharlotte\n28202\n4\n\n\n268\nwalter monahan\ncharlotte\n28205\n4\n\n\n1380\nkiea matthews\ncharlotte\n28218\n5\n\n\n252\nkiera matthews\ncharlotte\n28216\n5"
  },
  {
    "objectID": "posts/Deduplipy.html#advanced-deduplication",
    "href": "posts/Deduplipy.html#advanced-deduplication",
    "title": "Deduplication of records using DedupliPy",
    "section": "Advanced deduplication",
    "text": "Advanced deduplication\nLet’s explore some advanced settings to tailor the deduplicator to our needs. We are going to select the similarity metrics per field, define our own blocking rules and include interaction between the fields.\nThe similarity metrics per field are entered in a dict. Similarity metric can be any function that takes two strings and output a number. We use some string similarity functions that are implemented in the Python package called ‘thefuzz’ (pip install thefuzz):\n\nfrom thefuzz.fuzz import ratio, partial_ratio, token_set_ratio, token_sort_ratio\n\n\nfield_info = {'name':[ratio, partial_ratio], \n              'suburb':[token_set_ratio, token_sort_ratio], \n              'postcode':[ratio]}\n\nWe choose our own set of rules for blocking which we define ourselves. We only apply this rule to the ‘name’ column.\n\ndef first_two_characters(x):\n    return x[:2]\n\n\ninteraction=True makes the classifier include interaction features, e.g. ratio('name') * token_set_ratio('suburb'). When interaction features are included, the logistic regression classifier applies a L1 regularisation to prevent overfitting.\nWe set verbose=1 to get information on the progress and a distribution of scores\n\n\nmyDedupliPy = Deduplicator(field_info=field_info, interaction=True, rules={'name': [first_two_characters]}, verbose=1)\n\nFit the Deduplicator by active learning; enter whether a pair is a match (y) or not (n). When the training is converged, you will be notified and you can finish training by entering ‘f’.\n\nmyDedupliPy.fit(df)\n\nBased on the histogram of scores, we decide to ignore all pairs with a similarity probability lower than 0.1 when predicting:\nApply the trained Deduplicator on (new) data. The column deduplication_id is the identifier for a cluster. Rows with the same deduplication_id are found to be the same real world entity.\n\nres = myDedupliPy.predict(df, score_threshold=0.1)\nres.sort_values('deduplication_id').head(10)\n\nblocking started\nblocking finished\nNr of pairs: 27350\nscoring started\nscoring finished\nNr of filtered pairs: 892\nClustering started\nClustering finished\n\n\n\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\n1\nlucille richardst\nkannapolis\n28o81\n1\n\n\n1194\nlucille richards\nkannapolis\n28081\n1\n\n\n604\nlutta baldwin\nwhiteville\n28472\n3\n\n\n995\nlutta baldwin\nwhitevill\n28475\n3\n\n\n2\nreb3cca bauerboand\nraleigh\n27615\n5\n\n\n1134\nrebecca bauerband\nraleigh\n27615\n5\n\n\n1456\nrebecca harrell\nwinton\n27986\n7\n\n\n1024\nrebecca harrell\nwitnon\n27926\n7\n\n\n92\nrepecca harrell\nwinton\n27q86\n7\n\n\n675\nrebeccah shelton\nwhittier\n28789\n10"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science blog",
    "section": "",
    "text": "Welcome to my data science blog.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDeduplication of records using DedupliPy\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDistributed hyperparameter tuning of machine learning models in Spark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning.\nSome of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, repo)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (repo)\nPyMinHash - Minhashing in Python (repo)"
  },
  {
    "objectID": "about.html#frits-hermans",
    "href": "about.html#frits-hermans",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning.\nSome of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, repo)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (repo)\nPyMinHash - Minhashing in Python (repo)"
  }
]