[
  {
    "objectID": "hard_to_classify_datapoints.html",
    "href": "hard_to_classify_datapoints.html",
    "title": "Hard to classify datapoints",
    "section": "",
    "text": "|      |   y |   proba |\n|-----:|----:|--------:|\n|  460 |   1 |    0.83 |\n| 1287 |   1 |    0.84 |\n|  674 |   1 |    0.86 |\n| 1468 |   1 |    0.9  |\n|  916 |   1 |    0.93 |\n\n\n\n\n\n\n\n\n\n\n\ny\nproba\n\n\n\n\n460\n1\n0.83\n\n\n1287\n1\n0.84\n\n\n674\n1\n0.86\n\n\n1468\n1\n0.90\n\n\n916\n0\n0.93\n\n\n\n\n\n\n\n\n\n|      |   y |   proba |\n|-----:|----:|--------:|\n|  460 |   1 |    0.83 |\n| 1287 |   1 |    0.84 |\n|  674 |   1 |    0.86 |\n| 1468 |   1 |    0.9  |\n|  916 |   0 |    0.93 |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny\nproba\n\n\n\n\n460\n1\n0.83\n\n\n1287\n1\n0.84\n\n\n674\n1\n0.86\n\n\n1468\n0\n0.90\n\n\n916\n0\n0.93\n\n\n\n\n\n\n\n\n\n|      |   y |   proba |\n|-----:|----:|--------:|\n|  460 |   1 |    0.83 |\n| 1287 |   1 |    0.84 |\n|  674 |   1 |    0.86 |\n| 1468 |   0 |    0.9  |\n|  916 |   0 |    0.93 |"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#introduction",
    "href": "posts/hyperparameter_tuning_spark.html#introduction",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Introduction",
    "text": "Introduction\nHyperparameter tuning of machine learning models often requires significant computing time. Scikit-learn implements parallel processing to speed things up, but real speed gain can only be achieved by applying distributed computing like using Spark. In this blog post I show how to do hyperparameter tuning in Spark for any machine learning model, independent whether it’s scikit-learn, Tensorflow or Keras, XGBoost, LightGBM etc."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "href": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Create combinations of hyperparameter values",
    "text": "Create combinations of hyperparameter values\nFirst, we’re going to create hyperparameter combinations that we want to test our model for. Below is a helper functions to create all combinations for a param_grid that contains the arguments and the values to test for.\n\nimport numpy as np\nfrom itertools import product\n\n\ndef create_hyperparameter_combinations(param_grid):\n    combinations = list(product(*param_grid.values()))\n    return [dict(zip(param_grid.keys(), x)) for x in combinations]\n\nSo if our desired hyperparameter space is as follows…\n\nparam_grid = {'max_features': ['auto', 0.1, 0.3], 'min_samples_leaf': [None, 50]}\n\n…then the combination of these two hyperparameters and their values is obtained by`:\n\ncreate_hyperparameter_combinations(param_grid)\n\n[{'max_features': 'auto', 'min_samples_leaf': None},\n {'max_features': 'auto', 'min_samples_leaf': 50},\n {'max_features': 0.1, 'min_samples_leaf': None},\n {'max_features': 0.1, 'min_samples_leaf': 50},\n {'max_features': 0.3, 'min_samples_leaf': None},\n {'max_features': 0.3, 'min_samples_leaf': 50}]"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "href": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Create evaluation function",
    "text": "Create evaluation function\nNow, we’re going to create a function that takes a single combination of hyperparameter values and returns performance metrics for train and test data. This function will be evaluated in distributed fashion on our Spark cluster. If you want to test 500 different hyperparameter combinations, you will see 500 jobs being executed by Spark. In this example , were going to optimise hyperparameters for a scikit-learn model. This requires scikit-learn to be installed on the worker nodes of your Spark cluster.\nThe hyperparameter values for a particular combination are provided to the function as a json, so a string type. The advantage of this is that we don’t need to change the function definition if we want to add hyperparameters. Moreover, we are independent of the type of hyperparameter values. Many hyperparameters in scikit-learn take different types like integers, floats and strings like for example max_features in the RandomForestClassifier. When we provide them as a string containing a json, Spark never complains.\nThe function definition is as follows, explanation continues below.\n\nimport json\n\nfrom sklearn.model_selection import cross_validate\n\ndef evaluate_clf(base_clf, hyperpars_json, X, y, cv=5):\n    hyperpars = json.loads(hyperpars_json)\n    base_clf.set_params(**hyperpars)\n    cv_results = cross_validate(base_clf, X, y, cv=cv, return_train_score=True)\n    return (hyperpars_json,\n            float(np.mean(cv_results['train_score'])),\n            float(np.mean(cv_results['test_score'])))\n\nIn the first line, we convert the single combination of hyperparameter values to a Python dict by using json.loads. The next line sets these parameters in our scikit-learn model called base_clf. The scikit-learn function cross_validate takes the model, our training data features X and target y to produce train and test scores using cross validation. This function returns the results in a dict of which we take the train_score and test_score values that we return along with the hyperpars_json that we entered. Since Spark sometimes has difficulties with the np.float64 type, we convert the scores to the float type."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "href": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Distribute the evaluation function in Spark",
    "text": "Distribute the evaluation function in Spark\nThe next step is to distribute the hyperparameter combinations and use our evaluation function to calculate model performance metrics for these hyperparameter values. This is done in the function below.\n\nfrom pyspark.sql import types as T\n\ndef get_hyperparameter_results(spark_session, base_clf, X, y, hyperpar_combinations, cv=5):\n    hyperpar_combinations_json = [json.dumps(x) for x in hyperpar_combinations]\n    hyperpars_rdd = spark_session.sparkContext.parallelize(hyperpar_combinations_json, len(hyperpar_combinations_json))\n\n    rdd_map_result = hyperpars_rdd.map(lambda x: evaluate_clf(base_clf, x, X, y, cv))\n\n    result_schema = T.StructType([T.StructField('hyperpars', T.StringType()),\n                                  T.StructField('mean_train_score', T.FloatType()),\n                                  T.StructField('mean_test_score', T.FloatType()),\n                                  ])\n\n    result_sdf = spark_session.createDataFrame(rdd_map_result, schema=result_schema)\n    result = result_sdf.toPandas().sort_values('mean_test_score', ascending=False).reset_index(drop=True)\n\n    result['hyperpars'] = result['hyperpars'].apply(json.loads)\n    return result\n\nIn the first line we convert the dicts to json strings. Then, we parallelize these jsons by creating an RDD. The evaluate_clf can is mapped to this RDD. The schema of the result is defined as a StructType containing StructFields. Then a Spark dataframe is created that hold the results of our hyperparameter tuning. We convert this Spark dataframe to a Pandas dataframe in order to explore the results easily. As a last step we convert the jsons back to dicts."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "href": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "title": "Distributed hyperparameter tuning of machine learning models in Spark",
    "section": "Full working example",
    "text": "Full working example\nIn the example below we do hyperparameter tuning of a DecisionTreeClassifier to predict classes for the famous iris dataset.\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_iris(return_X_y=True)\n\nparam_grid = {'max_depth':[3,4,5,10], 'min_samples_leaf':[0.1, 5, 10]}\n\nhyperpar_combinations = create_hyperparameter_combinations(param_grid)\n\nresults = get_hyperparameter_results(spark, DecisionTreeClassifier(), X, y, hyperpar_combinations, cv=5)\nresults\n\n\n\n\n\n\n\n\nhyperpars\nmean_train_score\nmean_test_score\n\n\n\n\n0\n{'max_depth': 3, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n1\n{'max_depth': 4, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n2\n{'max_depth': 5, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n3\n{'max_depth': 10, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n4\n{'max_depth': 3, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n5\n{'max_depth': 3, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n6\n{'max_depth': 4, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n7\n{'max_depth': 4, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n8\n{'max_depth': 5, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n9\n{'max_depth': 5, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n10\n{'max_depth': 10, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n11\n{'max_depth': 10, 'min_samples_leaf': 10}\n0.961667\n0.933333"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science blog",
    "section": "",
    "text": "Welcome to my data science blog.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDistributed hyperparameter tuning of machine learning models in Spark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning. Some of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, repo)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (repo)\nPyMinHash - Minhashing in Python (repo)"
  }
]