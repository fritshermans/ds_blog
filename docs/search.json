[
  {
    "objectID": "hard_to_classify_datapoints.html",
    "href": "hard_to_classify_datapoints.html",
    "title": "Hard to classify datapoints in imbalanced data problems",
    "section": "",
    "text": "from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nrandom_state = 1\n\nX, y = make_classification(n_samples=5_000, weights=[0.95, 0.05], \n                           random_state=random_state)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    stratify=y, \n                                                    random_state=random_state)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = (RandomForestClassifier(class_weight='balanced', random_state=random_state)\n       .fit(X_train, y_train))\n\nprobas = clf.predict_proba(X_test)[:,1]\n\ndf = pd.DataFrame({'y':y_test, 'proba':probas})\n\n\n\n\n\n\nFigure 1: Precision recall curve\n\n\n\n\n\ndef flip_label_highest_score(df):\n    idx_to_change = df[df['y']==1].sort_values('proba').index[-1]\n    df.loc[idx_to_change, 'y'] = 0\n    return df\n\n\ndf = flip_label_highest_score(df)\n\n\n\nTable 1: Main Caption\n\n\n\n\n(a) Table 1\n\n\ny\nproba\n\n\n\n\n1\n0.83\n\n\n1\n0.84\n\n\n1\n0.86\n\n\n1\n0.90\n\n\n1\n0.93\n\n\n\n\n\n\n(b) Table 2\n\n\ny\nproba\n\n\n\n\n1\n0.83\n\n\n1\n0.84\n\n\n1\n0.86\n\n\n1\n0.90\n\n\n0\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) As is\n\n\n\n\n\n\n\n(b) After flipping label of point with largest score from 1 to 0\n\n\n\n\nFigure 2: Precision recall curves\n\n\n\ndf = flip_label_highest_score(df)\n\n\n\nTable 2: Main Caption\n\n\n\n\n(a) Table 1\n\n\ny\nproba\n\n\n\n\n1\n0.83\n\n\n1\n0.84\n\n\n1\n0.86\n\n\n1\n0.90\n\n\n1\n0.93\n\n\n\n\n\n\n(b) Table 2\n\n\ny\nproba\n\n\n\n\n1\n0.83\n\n\n1\n0.84\n\n\n1\n0.86\n\n\n1\n0.90\n\n\n0\n0.93\n\n\n\n\n\n\n(c) Table 3\n\n\ny\nproba\n\n\n\n\n1\n0.83\n\n\n1\n0.84\n\n\n1\n0.86\n\n\n0\n0.90\n\n\n0\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) As is\n\n\n\n\n\n\n\n(b) After flipping label of point with largest score from 1 to 0\n\n\n\n\n\n\n\n(c) After flipping the next point\n\n\n\n\nFigure 3: Precision recall curves"
  },
  {
    "objectID": "posts/PyMinhash.html",
    "href": "posts/PyMinhash.html",
    "title": "Finding duplicate records using PyMinHash",
    "section": "",
    "text": "MinHashing is a very efficient way of finding similar records in a dataset based on Jaccard similarity. My Python package PyMinHash implements efficient minhashing for Pandas dataframes. In this post I explain how to use PyMinHash. You can find more information on how the minhashing algorithm works here."
  },
  {
    "objectID": "posts/PyMinhash.html#installation",
    "href": "posts/PyMinhash.html#installation",
    "title": "Finding duplicate records using PyMinHash",
    "section": "Installation",
    "text": "Installation\nInstall PyMinHash from Pypi in the command line as follows:\npip install pyminhash"
  },
  {
    "objectID": "posts/PyMinhash.html#apply-on-pandas-dataframe",
    "href": "posts/PyMinhash.html#apply-on-pandas-dataframe",
    "title": "Finding duplicate records using PyMinHash",
    "section": "Apply on Pandas dataframe",
    "text": "Apply on Pandas dataframe\nPyMinHash comes with a toy dataset containing various name and address combinations of Stoxx50 companies. Let’s load the data and see what it contains.\n\nfrom pyminhash.datasets import load_data\ndf = load_data()\n\n\n\n\n\n\nname\n\n\n\n\nadidas ag adi dassler strasse 1 91074 germany\n\n\nadidas ag adi dassler strasse 1 91074 herzogenaurach\n\n\nadidas ag adi dassler strasse 1 91074 herzogenaurach germany\n\n\nairbus se 2333 cs leiden netherlands\n\n\nairbus se 2333 cs netherlands\n\n\n\n\n\nWe’re going to match various representations that belong to the same company. For this, we import create a MinHash object and tell it to use 10 hash tables. More hash tables means more accurate Jaccard similarity calculation but also requires more time and memory.\n\nfrom pyminhash.pyminhash import MinHash\nmyHasher = MinHash(n_hash_tables=10)\n\nThe fit_predict method needs the dataframe and the name of the column to which minhashing should be applied. The result is a dataframe containing all pairs that have a non-zero Jaccard similarity:\n\nresult = myHasher.fit_predict(df, 'name')\n\n\n\n\n\n\nname_1\nname_2\njaccard_sim\n\n\n\n\nengie sa 1 place samuel de champlain 92400 courbevoie\nengie sa 1 place samuel de champlain 92400 france\n1.0\n\n\nkoninklijke philips n v amstelplein 2 1096 bc\nkoninklijke philips n v amstelplein 2 1096 bc amsterdam\n1.0\n\n\nasml holding n v de run 6501 5504 dr veldhoven netherlands\nasml holding n v de run 6501 veldhoven netherlands\n1.0\n\n\namadeus it group s a salvador de madariaga 1 28027 madrid\namadeus it group s a salvador de madariaga 1 28027 madrid spain\n1.0\n\n\ndeutsche telekom ag 53113 bonn germany\ndeutsche telekom ag 53113 germany\n1.0\n\n\n\n\n\nAs one can see below, for a Jaccard similarity of 1.0, all words in the shortest string appear in the longest string. For lower Jaccard similarity values, the match is less than perfect. Note that Jaccard similarity has granularity of 1/n_hash_tables, in this example 0.1.\n\n\n\n\n\nname_1\nname_2\njaccard_sim\n\n\n\n\nengie sa 1 place samuel de champlain 92400 courbevoie\nengie sa 1 place samuel de champlain 92400 france\n1.0\n\n\nkoninklijke philips n v amstelplein 2 1096 bc\nkoninklijke philips n v amstelplein 2 1096 bc amsterdam\n1.0\n\n\nvinci sa 1 cours ferdinand de lesseps 92851 france\nvinci sa 1 cours ferdinand de lesseps rueil malmaison france\n0.9\n\n\nsanofi 54 rue la boetie 75008 france\nsanofi 54 rue la boetie 75008 paris france\n0.9\n\n\nairbus se 2333 cs leiden netherlands\nairbus se 2333 cs netherlands\n0.8\n\n\nfresenius se co kgaa else kroner strasse 1 61352 bad homburg vor der hohe germany\nfresenius se co kgaa else kroner strasse 1 61352 germany\n0.8\n\n\nbayerische motoren werke aktiengesellschaft munich germany\nbayerische motoren werke aktiengesellschaft petuelring 130 80788 munich\n0.7\n\n\nbayerische motoren werke aktiengesellschaft munich germany\nbayerische motoren werke aktiengesellschaft petuelring 130 munich germany\n0.7\n\n\naxa sa 75008 paris france\nsanofi 54 rue la boetie 75008 paris france\n0.6\n\n\nbanco bilbao vizcaya argentaria s a 48005 bilbao spain\nbanco bilbao vizcaya argentaria s a bilbao\n0.6\n\n\nlvmh moet hennessy louis vuitton societe europeenne 75008 france\nsanofi 54 rue la boetie 75008 paris france\n0.5\n\n\nsafran sa 2 boulevard du general martial valin paris france\nsafran sa 75724 france\n0.5\n\n\nmunchener ruckversicherungs gesellschaft aktiengesellschaft koniginstrasse 107 munich germany\nsiemens aktiengesellschaft munich germany\n0.4\n\n\nallianz se koniginstrasse 28 munich germany\nmunchener ruckversicherungs gesellschaft aktiengesellschaft koniginstrasse 107 munich germany\n0.4\n\n\nbnp paribas sa 16 boulevard des italiens 75009 france\nsociete generale societe 75009 paris france\n0.3\n\n\nessilorluxottica 1 6 rue paul cezanne paris france\nl oreal s a 92117 france\n0.3\n\n\nbnp paribas sa paris\nengie sa 92400 courbevoie france\n0.2\n\n\nessilorluxottica 1 6 rue paul cezanne paris france\nschneider electric s e 92500 france\n0.2\n\n\nsociete generale societe 75009 paris france\ntotal s a 2 place jean millier paris france\n0.1\n\n\nvinci sa 1 cours ferdinand de lesseps rueil malmaison france\nvivendi sa 75380 paris\n0.1\n\n\n\n\n\nIf you increase the number of hash tables, the accuracy of the Jaccard similarity increases but it comes at the cost of speed and memory usage."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#introduction",
    "href": "posts/hyperparameter_tuning_spark.html#introduction",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Introduction",
    "text": "Introduction\nHyperparameter tuning of machine learning models often requires significant computing time. Scikit-learn implements parallel processing to speed things up, but real speed gain can only be achieved by applying distributed computing like using Spark. In this blog post I show how to do hyperparameter tuning in Spark for any machine learning model, independent whether it’s Scikit-learn, Tensorflow/Keras, XGBoost, LightGBM etc."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "href": "posts/hyperparameter_tuning_spark.html#create-combinations-of-hyperparameter-values",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Create combinations of hyperparameter values",
    "text": "Create combinations of hyperparameter values\nFirst, we’re going to create hyperparameter combinations that we want to test our model for. Below is a helper functions to create all combinations for a param_grid that contains the arguments and the values to test for.\n\nimport numpy as np\nfrom itertools import product\n\n\ndef create_hyperparameter_combinations(param_grid):\n    combinations = list(product(*param_grid.values()))\n    return [dict(zip(param_grid.keys(), x)) for x in combinations]\n\nSo if our desired hyperparameter space is as follows…\n\nparam_grid = {'max_features': ['auto', 0.1, 0.3], 'min_samples_leaf': [None, 50]}\n\n…then the combination of these two hyperparameters and their values is obtained by:\n\ncreate_hyperparameter_combinations(param_grid)\n\n[{'max_features': 'auto', 'min_samples_leaf': None},\n {'max_features': 'auto', 'min_samples_leaf': 50},\n {'max_features': 0.1, 'min_samples_leaf': None},\n {'max_features': 0.1, 'min_samples_leaf': 50},\n {'max_features': 0.3, 'min_samples_leaf': None},\n {'max_features': 0.3, 'min_samples_leaf': 50}]"
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "href": "posts/hyperparameter_tuning_spark.html#create-evaluation-function",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Create evaluation function",
    "text": "Create evaluation function\nNow, we’re going to create a function that takes a single combination of hyperparameter values and returns performance metrics of the model with these hyperparameters for train and test data. This function will be evaluated in distributed fashion on our Spark cluster. If you want to test 500 different hyperparameter combinations, you will see 500 jobs being executed by Spark. In this example, we’re going to optimise hyperparameters for a Scikit-learn model. This requires Scikit-learn to be installed on the worker nodes of your Spark cluster.\nThe hyperparameter values for a particular combination are provided to the function as a json, so a string type. The advantage of this is that we don’t need to change the function definition if we want to add hyperparameters. Moreover, we are independent of the type of hyperparameter values. Many hyperparameters in Scikit-learn take different types like integers, floats and strings like for example max_features in the RandomForestClassifier. When we provide them as a string containing a json, Spark never complains.\nThe function definition is as follows, explanation continues below.\n\nimport json\n\nfrom sklearn.model_selection import cross_validate\n\ndef evaluate_clf(base_clf, hyperpars_json, X, y, cv=5):\n    hyperpars = json.loads(hyperpars_json)\n    base_clf.set_params(**hyperpars)\n    cv_results = cross_validate(base_clf, X, y, cv=cv, return_train_score=True)\n    return (hyperpars_json,\n            float(np.mean(cv_results['train_score'])),\n            float(np.mean(cv_results['test_score'])))\n\nIn the first line, we convert the single combination of hyperparameter values to a Python dict by using json.loads. The next line sets these parameters in our Scikit-learn model called base_clf. The Scikit-learn function cross_validate takes the model, our training data features X and target y to produce train and test scores using cross validation. This function returns the results in a dict of which we take the train_score and test_score values that we return along with the hyperpars_json that we entered. Since Spark sometimes has difficulties with the np.float64 type, we convert the scores to the float type."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "href": "posts/hyperparameter_tuning_spark.html#distribute-the-evaluation-function-in-spark",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Distribute the evaluation function in Spark",
    "text": "Distribute the evaluation function in Spark\nThe next step is to distribute the hyperparameter combinations and use our evaluation function to calculate model performance metrics for these hyperparameter values. This is done in the function below.\n\nfrom pyspark.sql import types as T\n\ndef get_hyperparameter_results(spark_session, base_clf, X, y, \n                               hyperpar_combinations, cv=5):\n    hyperpar_combinations_json = [json.dumps(x) for x in hyperpar_combinations]\n    hyperpars_rdd = spark_session.sparkContext.parallelize(hyperpar_combinations_json, \n                                                           len(hyperpar_combinations_json))\n\n    rdd_map_result = hyperpars_rdd.map(lambda x: evaluate_clf(base_clf, x, X, y, cv))\n\n    result_schema = T.StructType([T.StructField('hyperpars', T.StringType()),\n                                  T.StructField('mean_train_score', T.FloatType()),\n                                  T.StructField('mean_test_score', T.FloatType()),\n                                  ])\n\n    result_sdf = spark_session.createDataFrame(rdd_map_result, schema=result_schema)\n    result = (result_sdf.toPandas()\n              .sort_values('mean_test_score', ascending=False)\n              .reset_index(drop=True))\n\n    result['hyperpars'] = result['hyperpars'].apply(json.loads)\n    return result\n\nIn the first line we convert the dicts to json strings. Then, we parallelize these jsons by creating an RDD. The evaluate_clf can is mapped to this RDD. The schema of the result is defined as a StructType containing StructFields. Then a Spark dataframe is created that hold the results of our hyperparameter tuning. We convert this Spark dataframe to a Pandas dataframe in order to explore the results easily. As a last step we convert the jsons back to dicts."
  },
  {
    "objectID": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "href": "posts/hyperparameter_tuning_spark.html#full-working-example",
    "title": "Distributed hyperparameter tuning of Scikit-learn models in Spark",
    "section": "Full working example",
    "text": "Full working example\nIn the example below we do hyperparameter tuning of a DecisionTreeClassifier to predict classes for the famous iris dataset.\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_iris(return_X_y=True)\n\nparam_grid = {'max_depth':[3,4,5,10], 'min_samples_leaf':[0.1, 5, 10]}\n\nhyperpar_combinations = create_hyperparameter_combinations(param_grid)\n\nresults = get_hyperparameter_results(spark, DecisionTreeClassifier(), X, y, \n                                     hyperpar_combinations, cv=5)\n\n\n\n\n\n\nhyperpars\nmean_train_score\nmean_test_score\n\n\n\n\n{'max_depth': 3, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 4, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 5, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 10, 'min_samples_leaf': 5}\n0.968333\n0.940000\n\n\n{'max_depth': 3, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 3, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n{'max_depth': 4, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 4, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n{'max_depth': 5, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 5, 'min_samples_leaf': 10}\n0.961667\n0.933333\n\n\n{'max_depth': 10, 'min_samples_leaf': 0.1}\n0.961667\n0.933333\n\n\n{'max_depth': 10, 'min_samples_leaf': 10}\n0.961667\n0.933333"
  },
  {
    "objectID": "posts/Deduplipy.html",
    "href": "posts/Deduplipy.html",
    "title": "Deduplication of records using DedupliPy",
    "section": "",
    "text": "Deduplication or entity resolution is the task to combine different representations of the same real world entity. The Python package DedupliPy implements deduplication using active learning. Active learning allows for rapid training without having to provide a large, manually labelled dataset. In this post I demonstrate how the package works and show more advanced settings. In case you want to apply entity resolution on large data in Spark, please have a look at Spark-Matcher, a package I developed together with two colleagues."
  },
  {
    "objectID": "posts/Deduplipy.html#installation",
    "href": "posts/Deduplipy.html#installation",
    "title": "Deduplication of records using DedupliPy",
    "section": "Installation",
    "text": "Installation\nDedupliPy can simply be installed from PyPi. Just type the following in the command line:\npip install deduplipy"
  },
  {
    "objectID": "posts/Deduplipy.html#simple-deduplication",
    "href": "posts/Deduplipy.html#simple-deduplication",
    "title": "Deduplication of records using DedupliPy",
    "section": "Simple deduplication",
    "text": "Simple deduplication\nDedupliPy comes with example data. We first load the ‘voters’ data that contains duplicate records:\n\nfrom deduplipy.datasets import load_data\n\ndf = load_data(kind='voters')\n\nColumn names: 'name', 'suburb', 'postcode'\n\n\nThis dataset contains names, suburbs and postcodes.\n\n\n\n\n\nname\nsuburb\npostcode\n\n\n\n\nkhimerc thomas\ncharlotte\n2826g\n\n\nlucille richardst\nkannapolis\n28o81\n\n\nreb3cca bauerboand\nraleigh\n27615\n\n\n\n\n\nCreate a Deduplicator instance and provide the column names to be used for deduplication:\n\nfrom deduplipy.deduplicator import Deduplicator\n\n\nmyDedupliPy = Deduplicator(['name', 'suburb', 'postcode'])\n\nFit the Deduplicator by active learning; enter whether a pair is a match (y) or not (n). When the training is converged, you will be notified and you can finish training by entering ‘f’.\n\nmyDedupliPy.fit(df)\n\nApply the trained Deduplicator on (new) data. The column deduplication_id is the identifier for a cluster. Rows with the same deduplication_id are found to be the same real world entity.\n\nres = myDedupliPy.predict(df)\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\ncaria macartney\ncharlotte\n28220\n1\n\n\ncarla macartney\ncharlotte\n28227\n1\n\n\nmartha safrit\ncha4lotte\n282l5\n2\n\n\nmartha safrit\ncharlotte\n28215\n2\n\n\njeanronel corbier\ncharlotte\n28213\n3\n\n\njeanronel corpier\ncharrlotte\n28213\n3\n\n\nmelissa kaltenbach\ncharlotte\n28211\n4\n\n\nmelissa kalteribach\ncharlotte\n28251\n4\n\n\nkiea matthews\ncharlotte\n28218\n5\n\n\nkiera matthews\ncharlotte\n28216\n5\n\n\n\n\n\nThe Deduplicator instance can be saved as a pickle file and be applied on new data after training:\n\nimport pickle\n\n\nwith open('mypickle.pkl', 'wb') as f:\n    pickle.dump(myDedupliPy, f)\n\n\nwith open('mypickle.pkl', 'rb') as f:\n    loaded_obj = pickle.load(f)\n\n\nres = loaded_obj.predict(df)\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\ncaria macartney\ncharlotte\n28220\n1\n\n\ncarla macartney\ncharlotte\n28227\n1\n\n\nmartha safrit\ncha4lotte\n282l5\n2\n\n\nmartha safrit\ncharlotte\n28215\n2\n\n\njeanronel corbier\ncharlotte\n28213\n3\n\n\njeanronel corpier\ncharrlotte\n28213\n3\n\n\nmelissa kaltenbach\ncharlotte\n28211\n4\n\n\nmelissa kalteribach\ncharlotte\n28251\n4\n\n\nkiea matthews\ncharlotte\n28218\n5\n\n\nkiera matthews\ncharlotte\n28216\n5"
  },
  {
    "objectID": "posts/Deduplipy.html#advanced-deduplication",
    "href": "posts/Deduplipy.html#advanced-deduplication",
    "title": "Deduplication of records using DedupliPy",
    "section": "Advanced deduplication",
    "text": "Advanced deduplication\nIf you’re intested in the inner workings of DedupliPy, please watch my presentation at PyData Global 2021:\n\nLet’s explore some advanced settings to tailor the deduplicator to our needs. We are going to select the similarity metrics per field, define our own blocking rules and include interaction between the fields.\nThe similarity metrics per field are entered in a dict. Similarity metric can be any function that takes two strings and output a number. We use some string similarity functions that are implemented in the Python package called ‘thefuzz’ (pip install thefuzz):\n\nfrom thefuzz.fuzz import ratio, partial_ratio, token_set_ratio, token_sort_ratio\n\n\nfield_info = {'name':[ratio, partial_ratio], \n              'suburb':[token_set_ratio, token_sort_ratio], \n              'postcode':[ratio]}\n\nWe choose a set of rules for blocking which we define ourselves. We only apply this rule to the ‘name’ column.\n\ndef first_two_characters(x):\n    return x[:2]\n\nWhen we setinteraction=True, the classifier includes interaction features, e.g. ratio('name') * token_set_ratio('suburb'). When interaction features are included, the logistic regression classifier applies a L1 regularisation to prevent overfitting. We also set verbose=1 to get information on the progress and a distribution of scores\n\nmyDedupliPy = Deduplicator(field_info=field_info, interaction=True, \n                           rules={'name': [first_two_characters]}, verbose=1)\n\nFit the Deduplicator by active learning; enter whether a pair is a match (y) or not (n). When the training is converged, you will be notified and you can finish training by entering ‘f’.\n\nmyDedupliPy.fit(df)\n\nAfter fitting, the histogram of scores is shown. Based on this histogram, we decide to ignore all pairs with a similarity probability lower than 0.1 when predicting:\nApply the trained Deduplicator on (new) data. The column deduplication_id is the identifier for a cluster. Rows with the same deduplication_id are found to be the same real world entity.\n\nres = myDedupliPy.predict(df, score_threshold=0.1)\n\n\n\n\n\n\nname\nsuburb\npostcode\ndeduplication_id\n\n\n\n\nlucille richardst\nkannapolis\n28o81\n1\n\n\nlucille richards\nkannapolis\n28081\n1\n\n\nlutta baldwin\nwhiteville\n28472\n3\n\n\nlutta baldwin\nwhitevill\n28475\n3\n\n\nrepecca harrell\nwinton\n27q86\n5\n\n\nrebecca harrell\nwinton\n27986\n5\n\n\nrebecca harrell\nwitnon\n27926\n5\n\n\nrebecca bauerband\nraleigh\n27615\n6\n\n\nreb3cca bauerboand\nraleigh\n27615\n6\n\n\nrebeccah shelton\nwhittier\n28789\n7"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science blog",
    "section": "",
    "text": "Welcome to my data science blog.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDeduplication of records using DedupliPy\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDistributed hyperparameter tuning of Scikit-learn models in Spark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFinding duplicate records using PyMinHash\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning.\nSome of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, GitHub, PyData Global presentation)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (GitHub)\nPyMinHash - Minhashing in Python (GitHub)\n\nOther:\n- LockdownRadar.nl (newspaper article)"
  },
  {
    "objectID": "about.html#frits-hermans",
    "href": "about.html#frits-hermans",
    "title": "About",
    "section": "",
    "text": "I work as a data scientist for a financial institution. My main topics of interest are entity resolution, fuzzy matching, classification for imbalanced data problems and aggregation learning.\nSome of the libraries I created or co-created:\n\nDeduplipy - Entity resolution package (deduplipy.com, GitHub, PyData Global presentation)\n\nSpark-Matcher - Entity resolution and fuzzy matching at scale in Spark (GitHub)\nPyMinHash - Minhashing in Python (GitHub)\n\nOther:\n- LockdownRadar.nl (newspaper article)"
  }
]